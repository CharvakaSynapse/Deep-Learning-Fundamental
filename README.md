# Deep Learning Fundamentals

Short, evolving notes on core deep-learning concepts. Expect concise write-ups; the topic list will keep expanding.

## Topics (growing)
- Convolutions: math ↔ practice (discrete/continuous, cross-corr vs conv, FFT/Winograd)
- Initialization (Xavier/He, signal propagation, scale/variance)
- Optimization (SGD/Adam, schedules, warmup, EMA)
- Normalization (Batch/Layer/Group; when to skip/decay)
- Regularization (weight decay, dropout, label smoothing)
- Data augmentation (MixUp, CutMix, RandAugment)
- Architectures (ResNets, ConvNeXt, ViT/Swin; 1×1, depthwise/grouped, dilated, deformable)
- Efficient inference (quantization, pruning, distillation)
- Training at scale (DDP/FSDP, AMP, checkpointing)
- MLOps notes (tracking, packaging, deployment)

## Layout
- `docs/` — short articles
- `notebooks/` — minimal demos
- `figures/` — diagrams

> WIP and updated regularly. Contributions and issues welcome.
